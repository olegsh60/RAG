{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olegsh60/RAG/blob/main/%D0%9F%D0%BE%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%B8%D0%B5_RAG_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D1%8B_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_LLM_Qwen_2_5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uF_KywH0yV9"
      },
      "source": [
        "#Ответы на вопросы с помощью LLM модели Qwen 7B + RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XTa2RoL8H6N"
      },
      "source": [
        "Устанавливаем общие библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk-K_mHMs7ns"
      },
      "outputs": [],
      "source": [
        "# Библиотеки Langchain для взаимодействия с LLM\n",
        "!pip install -U langchain langchain-community langchain-huggingface -q\n",
        "# Библиотеки для вектороной DB FAISS и загрузки PDF\n",
        "!pip install faiss-cpu pypdf -q\n",
        "# Следующие три строки - установка библиотеки unsloth для использования квантованных LLM.\n",
        "# Для Colab процесс имеет специфику по версиям библиотек. Если запускать не в Colab, то просто:\n",
        "# !pip install unsloth\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo -q\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLGWjfWPAoaL"
      },
      "source": [
        "##Алгоритм создания RAG системы\n",
        "1). Document Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GVoLe2bAnHO"
      },
      "outputs": [],
      "source": [
        "# Загружаем файл \"ГОСТ Р 56939-2016 - безопасность ПО.pdf\" для RAG, предварительно его нужно загрузить в Colab в раздел в левой пенели \"Files\"\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader('./ГОСТ Р 56939-2016 - безопасность ПО.pdf')\n",
        "documents = loader.load()\n",
        "print(\"Число страниц: \", len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAEFxcu3GO2q"
      },
      "source": [
        "2). Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P1baCK81MDz"
      },
      "outputs": [],
      "source": [
        "# Определите Splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "split_documents = splitter.split_documents(documents)\n",
        "\n",
        "print(\"Число чанков: \", len(split_documents))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt5IiEYNGvu7"
      },
      "source": [
        "3). Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBw73RuxGwgs"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "#  cuda - Use the GPU (if available)\n",
        "hf_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"cointegrated/LaBSE-en-ru\", model_kwargs={\"device\": \"cuda\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWMbyixJHMM4"
      },
      "source": [
        "4). Storage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEwxmvu9HMit"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db_embed = FAISS.from_documents(split_documents, hf_embeddings_model)\n",
        "db_embed.save_local(\"faiss_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RM4lTAwHdbq"
      },
      "source": [
        "5). Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p446j-8iHd6l"
      },
      "outputs": [],
      "source": [
        "# Используем векторноое хранилище и его методов для получения документов\n",
        "retriever = db_embed.as_retriever(\n",
        "    search_type=\"similarity\",  # тип поиска похожих документов\n",
        "    k=4,  # количество возвращаемых документов (Default: 4)\n",
        "    score_threshold=None,  # минимальный порог для поиска \"similarity_score_threshold\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8hlFSPLEzlB"
      },
      "source": [
        "## Работа RAG системы\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtTOt7TM3ERT"
      },
      "source": [
        "Импорт необходимых обьектов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMAz2Nx2aQQK"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.schema import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpEjQ7aH84qs"
      },
      "source": [
        "Будем использовать Qwen 2.5 с 7B параметрами квантованную  в 4bit\n",
        "https://huggingface.co/unsloth/Qwen2.5-7B-bnb-4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZMM0kcpaTQS"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bweqX9SA_RDC"
      },
      "source": [
        "Pipeline для формирования ответа LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok0hzm-JdlXd"
      },
      "outputs": [],
      "source": [
        "\n",
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\".\")]\n",
        "\n",
        "pipe = pipeline('text-generation',\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=150, # обьем возвращаемых токенов\n",
        "                repetition_penalty=1.2, # штрав за повторы в ответе немного увеличим от дефортной 1\n",
        "                temperature=0.7, # Креативность уменьшим от дефортной 1 чтобы получать более точные ответы\n",
        "                eos_token_id=terminators)\n",
        "\n",
        "HF_model = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvOf9Db1ISiU"
      },
      "source": [
        "Формируем шаблон для промпта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve--ff68Mw17"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Отвечай на вопрос только используя следующий контекст:\n",
        "{context}\n",
        "Если ответа нет в контексте, то ответь: я не знаю!\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Определение шаблона промпта\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "# Объявляем функцию, которая будет собирать строку из полученных документов\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgo3iVePAVEB"
      },
      "source": [
        "Запускаем RAG, очищаем ответ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PiJKssbISIf"
      },
      "outputs": [],
      "source": [
        "query = \"Что такое безопасное программное обеспечение?\"\n",
        "context = retriever.get_relevant_documents(query)\n",
        "context = format_docs(context)\n",
        "response = HF_model.invoke(prompt_template.format(question=query, context=context))\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "string_result = output_parser.parse(response)\n",
        "\n",
        "print(string_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5xohA7aLQhW"
      },
      "source": [
        "Пример вопроса на тему не из документа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fluB1FHLQ6K"
      },
      "outputs": [],
      "source": [
        "# Генерация ответа не по контексту\n",
        "query = \"Какое расстояние до луны?\"\n",
        "\n",
        "context = retriever.get_relevant_documents(query)\n",
        "context = format_docs(context)\n",
        "response = HF_model.invoke(prompt_template.format(question=query, context=context))\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "string_result = output_parser.parse(response)\n",
        "\n",
        "print(string_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6T2C1FFIPHb"
      },
      "source": [
        "Пример запроса к чистой модели без использования RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQSxe37JH1vQ"
      },
      "outputs": [],
      "source": [
        "# Определение шаблона промпта\n",
        "prompt_template_short = PromptTemplate.from_template(\"Вопрос: {question}\")\n",
        "\n",
        "# Генерация ответа\n",
        "query = \"Что такое безопасное программное обеспечение?\"\n",
        "response = HF_model.invoke(prompt_template_short.format(question=query))\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEMw6hIFBTVc"
      },
      "source": [
        "Пример вопроса к чистой модели на тему не из документа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NucgW_lcM-km"
      },
      "outputs": [],
      "source": [
        "# Определение шаблона промпта\n",
        "prompt_template_short = PromptTemplate.from_template(\"Вопрос: {question}\")\n",
        "\n",
        "# Генерация ответа\n",
        "query = \"Какое расстяние до луны?\"\n",
        "response = HF_model.invoke(prompt_template_short.format(question=query))\n",
        "\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CoofhZcDtp6R"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}